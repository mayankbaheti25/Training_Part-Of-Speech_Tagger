{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dependencies\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import pprint \n",
    "from nltk.corpus.reader.tagged import TaggedCorpusReader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location where your files are saved\n",
    "root = './/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the tagged corpus\n",
    "\n",
    "corpus = TaggedCorpusReader(root,\"train.txt\")\n",
    "tagged_sentences = corpus.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7791\n",
      "2597\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the dataset for training and testing\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "print(len(training_sentences))   # 2935\n",
    "print(len(test_sentences))         # 979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': True,\n",
      " 'is_capitalized': False,\n",
      " 'is_first': False,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'sentence',\n",
      " 'prefix-1': 'a',\n",
      " 'prefix-2': 'a',\n",
      " 'prefix-3': 'a',\n",
      " 'prev_word': 'is',\n",
      " 'suffix-1': 'a',\n",
      " 'suffix-2': 'a',\n",
      " 'suffix-3': 'a',\n",
      " 'word': 'a'}\n"
     ]
    }
   ],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" Compute some very basic word features.\n",
    "        :param sentence_terms: [w1, w2, ...]\n",
    "        :type sentence_terms: list\n",
    "        :param index: the index of the word\n",
    "        :type index: int\n",
    "        :return: dict containing features\n",
    "        :rtype: dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "\n",
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    \"\"\"\n",
    "    Remove the tag for each tagged term.\n",
    "    :param tagged_sentence: a POS tagged sentence\n",
    "    :type tagged_sentence: list\n",
    "    :return: a list of tags\n",
    "    :rtype: list of strings\n",
    "    \"\"\"\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    \"\"\"\n",
    "    Split tagged sentences to X and y datasets and append some basic features.\n",
    "    :param tagged_sentences: a list of POS tagged sentences\n",
    "    :param tagged_sentences: list of list of tuples (term_i, tag_i)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the data for to be used in the model\n",
    "X, y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165816"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)#number of records in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 21305), ('IN', 17167), ('AT', 14530), ('NP', 8965), ('JJ', 8203), ('NNS', 8187), (',', 8069), ('.', 7615), ('VB', 4655), ('CC', 4606), ('RB', 4097), ('VBN', 3857), ('NN-TL', 3692), ('VBD', 3286), ('CS', 2765), ('CD', 2743), ('VBG', 2327), ('TO', 2232), ('MD', 1935), ('PPS', 1889), ('PP$', 1860), ('AP', 1550), ('BEZ', 1513), ('PPSS', 1294), ('``', 1156), ('DT', 1127), (\"''\", 1112), ('VBZ', 1107), ('JJ-TL', 1107), ('NP-TL', 1101), ('QL', 1047), ('BEDZ', 1041), ('BE', 959), ('PPO', 897), ('RP', 725), ('WPS', 677), ('WDT', 663), ('BER', 638), ('WRB', 607), ('HVZ', 579), ('*', 576), ('HV', 557), ('NR', 554), ('NNS-TL', 542), ('--', 497), ('NP$', 447), ('OD', 412), ('DTI', 401), ('HVD', 396), ('BEN', 373), ('NN-HL', 372), ('BED', 362), ('ABN', 345), ('NPS', 334), ('NN$', 291), ('DTS', 281), ('NP-HL', 270), ('IN-TL', 267), ('EX', 265), (')', 250), ('JJR', 248), (':', 247), ('(', 247), ('PN', 205), ('IN-HL', 179), ('VBN-TL', 176), ('DO', 175), ('JJT', 169), ('NNS-HL', 142), ('RBR', 135), ('CD-TL', 131), ('AT-HL', 110), ('MD*', 107), ('BEG', 105), ('ABX', 105), ('DOD', 101), ('NN$-TL', 98), ('AT-TL', 97), ('PPL', 94), ('JJ-HL', 86), (',-HL', 81), ('NNS$', 73), ('JJS', 73), ('DOZ', 73), ('CD-HL', 69), ('CC-TL', 69), ('VBN-HL', 66), (\"'\", 65), ('PPS+BEZ', 57), ('VBZ-HL', 56), ('OD-TL', 49), (':-HL', 48), ('PPLS', 47), ('NR-TL', 46), ('DO*', 42), ('.-HL', 41), ('VB-HL', 40), ('PPSS+MD', 38), ('ABL', 37), ('FW-NN', 36), ('WP$', 35), ('HVG', 33), ('PPSS+BER', 32), ('BEM', 32), ('HVN', 30), ('UH', 28), ('NR$', 28), ('NNS$-TL', 28), ('VBG-TL', 27), ('QLP', 27), ('VBG-HL', 26), ('DOZ*', 26), ('PPSS+HV', 25), ('VB-TL', 24), ('RB-HL', 24), ('WPO', 23), ('NN-TL-HL', 23), ('FW-NN-TL', 21), ('PPSS+BEM', 20), ('DOD*', 20), (')-HL', 20), ('(-HL', 20), ('NP$-TL', 19), ('NPS-TL', 18), ('FW-IN', 18), ('NPS$', 17), ('BEZ*', 15), ('AP-HL', 15), ('FW-JJ-TL', 14), ('DTX', 14), ('DT+BEZ', 14), ('CC-HL', 14), ('PP$$', 13), ('FW-AT-TL', 13), ('RBT', 10), ('RB-TL', 10), ('PPS+MD', 10), ('MD-HL', 9), ('BER*', 9), ('TO-HL', 8), ('JJR-TL', 8), ('JJR-HL', 8), ('FW-NNS', 8), ('FW-JJ', 8), ('VBD-HL', 7), ('PPS+HVZ', 7), ('NNS-TL-HL', 7), ('HVZ*', 7), ('RP-HL', 6), ('EX+BEZ', 6), ('PP$-TL', 5), ('NPS-HL', 5), ('JJT-HL', 5), ('BEZ-HL', 5), ('BEDZ*', 5), ('ABN-TL', 5), ('WDT+BEZ', 4), ('RB$', 4), ('NR$-TL', 4), ('NP$-HL', 4), ('FW-AT', 4), ('CS-HL', 4), ('*-HL', 4), ('WRB-HL', 3), ('WDT-HL', 3), ('VBZ-TL', 3), ('RN', 3), ('PN$', 3), ('NRS', 3), ('NN-NC', 3), ('NN$-HL', 3), ('JJS-TL', 3), ('JJ-TL-HL', 3), ('DT-TL', 3), ('WQL', 2), ('WPS+BEZ', 2), ('VBN-TL-HL', 2), ('VB+PPO', 2), ('PPSS-TL', 2), ('PPSS-HL', 2), ('PPSS+HVD', 2), ('PPO-TL', 2), ('PPO-HL', 2), ('PN-HL', 2), ('NR-HL', 2), ('NP+BEZ', 2), ('NNS$-HL', 2), ('MD-TL', 2), ('HVD*', 2), ('FW-WDT', 2), ('FW-VB', 2), ('FW-IN-TL', 2), ('FW-IN+NN', 2), ('FW-CC', 2), ('DTI-HL', 2), ('DO-HL', 2), ('CD$', 2), ('BER-HL', 2), ('BE-HL', 2), ('AP-TL', 2), ('AP$', 2), ('WPO-TL', 1), ('WDT+BEZ-HL', 1), ('VBD-TL', 1), ('UH-TL', 1), ('UH-HL', 1), ('TO-TL', 1), ('RB-NC', 1), ('RB+BEZ', 1), ('QL-TL', 1), ('QL-HL', 1), ('PPS+BEZ-HL', 1), ('PPL-HL', 1), ('PP$-HL', 1), ('PN+HVZ', 1), ('OD-HL', 1), ('NPS$-TL', 1), ('NPS$-HL', 1), ('NP-TL-HL', 1), ('MD+HV', 1), ('MD*-HL', 1), ('JJR-NC', 1), ('JJ-NC', 1), ('IN-NC', 1), ('HVD-HL', 1), ('FW-VBZ', 1), ('FW-VB-NC', 1), ('FW-PPO', 1), ('FW-PPL', 1), ('FW-PP$-NC', 1), ('FW-NP', 1), ('FW-IN+NN-TL', 1), ('FW-IN+AT-TL', 1), ('FW-DT', 1), ('FW-CD', 1), ('FW-AT-HL', 1), ('FW-*', 1), ('DT-HL', 1), ('DT$', 1), ('DOZ-HL', 1), ('DO-TL', 1), ('BEZ-TL', 1), ('BER-TL', 1), ('BEDZ-HL', 1), ('BED*', 1), ('ABN-HL', 1), (':-TL', 1), ('---HL', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y).items(), key = lambda kv:(kv[1], kv[0]),reverse=True)) \n",
    "# The top five POS tags are: NN,IN, AT, NP, JJ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n",
      "Accuracy: 0.8469216373146504\n"
     ]
    }
   ],
   "source": [
    "#Model pipeline is created and the fitted on training data\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=20, criterion = 'entropy'))])\n",
    " \n",
    "clf.fit(X[:25000], y[:25000])   # Use only the first 25K samples if you're running it multiple times. It takes a fair bit :)\n",
    " \n",
    "print('Training completed')\n",
    " \n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    " \n",
    "print (\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8469216373146504, 0.8469216373146504, 0.8469216373146504, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "precision_recall_fscore_support(y_true, y_pred,average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to be used when we are predicting on the unseen on untagged data\n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return sentence, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the untagged data\n",
    "with open('test.txt', 'r') as myfile:\n",
    "    data = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all extra spaces\n",
    "data = \" \".join(data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the prediction on untagged data using the pos_tag function\n",
    "x_1,y_1 = pos_tag(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = TaggedCorpusReader(root,\"test.tag\")\n",
    "a,b = transform_to_dataset(corpus_test.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "898"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_1)\n",
    "# Now we have see that we donot have all the tags in the tagged file, hence we will be using the tokens from tagged \n",
    "# file to do the prediction and the calculate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8890134529147982, 0.8890134529147982, 0.8890134529147982, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = b\n",
    "y_pred = clf.predict(a)\n",
    "precision_recall_fscore_support(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8890134529147982\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy:\", clf.score(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = [i +'/'+ j for i, j in zip(x_1, list(y_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out_String = \" \".join(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"test.out\",\"w\")\n",
    "file1.writelines(Out_String) \n",
    "file1.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
